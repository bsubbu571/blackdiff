results/petastorm/docs/autodoc/conf.py:134 - 'github_user': 'uber',
results/petastorm/petastorm/reader.py:109 - :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to
results/petastorm/petastorm/reader.py:112 - pass in a unique shard number in the range [0, shard_count). shard_count must be supplied as well.
results/petastorm/petastorm/reader.py:123 - :param cache_extra_settings: A dictionary of extra settings to pass to the cache implementation,
results/petastorm/petastorm/reader.py:261 - :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to
results/petastorm/petastorm/reader.py:264 - pass in a unique shard number in the range [0, shard_count). shard_count must be supplied as well.
results/petastorm/petastorm/reader.py:275 - :param cache_extra_settings: A dictionary of extra settings to pass to the cache implementation,
results/petastorm/petastorm/reader.py:385 - :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to
results/petastorm/petastorm/reader.py:388 - pass in a unique shard number in the range ``[0, shard_count)``.
results/petastorm/petastorm/reader.py:500 - will be raised if a user attempt to do so.
results/petastorm/petastorm/reader.py:535 - pass in a unique partition number in the range [0, shard_count).
results/petastorm/petastorm/transform.py:29 - """TransformSpec defines a user transformation that is applied to a loaded row on a worker thread/process.
results/petastorm/petastorm/transform.py:39 - complies to the input schema and must return a dictionary that complies to a post-transform schema. User may
results/petastorm/petastorm/transform.py:40 - In case the user wants to only remove certain fields, the user may omit this argument and specify only
results/petastorm/petastorm/transform.py:48 - Note: For param `removed_fields` and `selected_fields`, user can only specify one of them.
results/petastorm/petastorm/transform.py:54 - raise ValueError('User can only specify one of removed_fields and selected_fields in TransformSpec.')
results/petastorm/petastorm/tf_utils.py:239 - # Pass py_func output via shuffling queue if requested.
results/petastorm/petastorm/tf_utils.py:262 - # Pass py_func output via shuffling queue if requested.
results/petastorm/petastorm/pytorch.py:98 - _PARALLEL_ITER_ERROR = "You must finish a full pass of Petastorm DataLoader before making another pass from the \
results/petastorm/petastorm/pytorch.py:117 - logger.warning('Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.')
results/petastorm/petastorm/pytorch.py:149 - returned to a user by the ``DataLoader`` until the queue is full. After that, batches of `batch_size`
results/petastorm/petastorm/pytorch.py:274 - returned to a user by the ``BatchedDataLoader`` until the queue is full. After that, batches of `batch_size`
results/petastorm/petastorm/arrow_reader_worker.py:194 - # A user may omit `func` value if they intend just to delete some fields using the TransformSpec
results/petastorm/petastorm/fs_utils.py:45 - hdfs_driver='libhdfs3', user=None, storage_options=None):
results/petastorm/petastorm/fs_utils.py:58 - 5. If ``scheme`` is ``s3``, use s3fs. The user must manually install s3fs before using s3
results/petastorm/petastorm/fs_utils.py:59 - 6. If ``scheme`` is ``gs``or ``gcs``, use gcsfs. The user must manually install gcsfs before using GCS
results/petastorm/petastorm/fs_utils.py:67 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/fs_utils.py:106 - self._filesystem = connector.connect_to_either_namenode(namenodes, user=user)
results/petastorm/petastorm/fs_utils.py:107 - self._filesystem_factory = lambda: connector.connect_to_either_namenode(namenodes, user=user)
results/petastorm/petastorm/fs_utils.py:110 - self._filesystem = connector.hdfs_connect_namenode(self._parsed_dataset_url, user=user)
results/petastorm/petastorm/fs_utils.py:112 - lambda url=self._dataset_url, user=user: \
results/petastorm/petastorm/fs_utils.py:113 - connector.hdfs_connect_namenode(urlparse(url), user=user)
results/petastorm/petastorm/fs_utils.py:118 - filesystem = connector.connect_to_either_namenode(namenodes, user=user)
results/petastorm/petastorm/fs_utils.py:119 - self._filesystem_factory = lambda: connector.connect_to_either_namenode(namenodes, user=user)
results/petastorm/petastorm/fs_utils.py:126 - self._filesystem = connector.hdfs_connect_namenode(self._parsed_dataset_url, hdfs_driver, user=user)
results/petastorm/petastorm/fs_utils.py:128 - lambda url=self._dataset_url, user=user: \
results/petastorm/petastorm/fs_utils.py:129 - connector.hdfs_connect_namenode(urlparse(url), hdfs_driver, user=user)
results/petastorm/petastorm/namedtuple_gt_255_fields.py:122 - # Validate the field names.  At the user's option, either generate an error
results/petastorm/petastorm/namedtuple_gt_255_fields.py:178 - # where the named tuple is created.  Bypass this step in environments where
results/petastorm/petastorm/namedtuple_gt_255_fields.py:180 - # defined for arguments greater than 0 (IronPython), or where the user has
results/petastorm/petastorm/tests/test_tf_dataset.py:104 - ``Reader initialization``. A user should use ``Reader`` built-in epochs support, or use
results/petastorm/petastorm/tests/test_fs_utils.py:89 - user=self.mock_name)
results/petastorm/petastorm/tests/test_fs_utils.py:103 - user=self.mock_name)
results/petastorm/petastorm/tests/test_fs_utils.py:121 - user=self.mock_name)
results/petastorm/petastorm/tests/test_fs_utils.py:135 - connector=self.mock, hdfs_driver='libhdfs', user=self.mock_name)
results/petastorm/petastorm/tests/test_fs_utils.py:147 - connector=self.mock, user=self.mock_name)
results/petastorm/petastorm/tests/test_fs_utils.py:161 - connector=self.mock, user=self.mock_name)
results/petastorm/petastorm/tests/test_run_in_subprocess.py:44 - # Must pass as positional arg in the right order
results/petastorm/petastorm/tests/test_pytorch_dataloader.py:253 - match_str = 'You must finish a full pass of Petastorm DataLoader before making another pass from the beginning'
results/petastorm/petastorm/reader_impl/pytorch_shuffling_buffer.py:78 - This allows a user to deplete the buffer. Typically during last epoch. Otherwise, we would always have leftovers
results/petastorm/petastorm/reader_impl/shuffling_buffer.py:68 - This allows a user to deplete the buffer. Typically during last epoch. Otherwise, we would always have leftovers
results/petastorm/petastorm/workers_pool/process_pool.py:221 - # exhausted which the user can decide how to handle instead of just having the process hang.
results/petastorm/petastorm/workers_pool/tests/test_ventilator.py:52 - """Tests that we dont surpass a max ventilation size in each pool type
results/petastorm/petastorm/workers_pool/tests/test_workers_pool.py:36 - """Pass a coefficient to the workers and make it multiply the input with this coefficient"""
results/petastorm/petastorm/tools/copy_dataset.py:54 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/tools/copy_dataset.py:71 - hdfs_driver=hdfs_driver, user=spark.sparkContext.sparkUser())
results/petastorm/petastorm/hdfs/namenode.py:213 - def __init__(self, connector_cls, list_of_namenodes, user=None):
results/petastorm/petastorm/hdfs/namenode.py:220 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/hdfs/namenode.py:226 - self._user = user
results/petastorm/petastorm/hdfs/namenode.py:238 - self._connector_cls._try_next_namenode(self._index_of_nn, self._list_of_namenodes, user=self._user)
results/petastorm/petastorm/hdfs/namenode.py:245 - def hdfs_connect_namenode(cls, url, driver='libhdfs3', user=None):
results/petastorm/petastorm/hdfs/namenode.py:251 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/hdfs/namenode.py:257 - # So we pass 'default' as a host name if the url does not specify one (i.e. hdfs:///...)
results/petastorm/petastorm/hdfs/namenode.py:265 - kwargs = dict(user=user)
results/petastorm/petastorm/hdfs/namenode.py:273 - def connect_to_either_namenode(cls, list_of_namenodes, user=None):
results/petastorm/petastorm/hdfs/namenode.py:281 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/hdfs/namenode.py:285 - return HAHdfsClient(cls, list_of_namenodes, user=user)
results/petastorm/petastorm/hdfs/namenode.py:288 - def _try_next_namenode(cls, index_of_nn, list_of_namenodes, user=None):
results/petastorm/petastorm/hdfs/namenode.py:295 - :param user: String denoting username when connecting to HDFS. None implies login user.
results/petastorm/petastorm/hdfs/namenode.py:308 - cls.hdfs_connect_namenode(urlparse('hdfs://' + str(host or 'default')), user=user)
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:271 - def __init__(self, n_failovers=0, user=None):
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:273 - self._user = user
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:337 - def hdfs_connect_namenode(cls, url, driver='libhdfs3', user=None):
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:352 - hdfs = MockHdfs(cls._n_failovers, user=user)
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:380 - mocked_hdfs = self.suj.connect_to_either_namenode(self.NAMENODES, user=mock_name)
results/petastorm/petastorm/hdfs/tests/test_hdfs_namenode.py:462 - client = HAHdfsClient(MockHdfsConnector, self.NAMENODES, user=mock_name)
results/petastorm/petastorm/etl/rowgroup_indexing.py:53 - hdfs_driver=hdfs_driver, user=spark_context.sparkUser())
results/petastorm/petastorm/etl/dataset_metadata.py:73 - A user may provide their own recipe for creation of pyarrow filesystem object in ``filesystem_factory``
results/petastorm/petastorm/etl/dataset_metadata.py:100 - user=spark.sparkContext.sparkUser())
results/petastorm/petastorm/etl/petastorm_generate_metadata.py:59 - :param user: String denoting username when connecting to HDFS
results/petastorm/petastorm/etl/petastorm_generate_metadata.py:64 - user=spark.sparkContext.sparkUser())
results/petastorm/petastorm/spark/spark_dataset_converter.py:220 - :param num_epochs: An epoch is a single pass over all rows in the dataset.
results/petastorm/petastorm/spark/spark_dataset_converter.py:264 - :param num_epochs: An epoch is a single pass over all rows in the
results/petastorm/petastorm/spark/spark_dataset_converter.py:479 - # User need to use dbfs fuse URL.
results/petastorm/petastorm/spark/spark_dataset_converter.py:582 - This will help user to find the related spark application for a directory.
results/petastorm/petastorm/spark/spark_dataset_converter.py:583 - So that if atexit deletion failed, user can manually delete them.
results/petastorm/examples/mnist/pytorch_example.py:98 - # data or target transform, but that actually gives the user more flexibility
results/petastorm/.github/workflows/unittest.yml:141 - GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # This token is provided by Actions, you do not need to create your own token
results/petastorm/.github/workflows/unittest.yml:174 - user: __token__
results/petastorm/.github/workflows/unittest.yml:175 - password: ${{ secrets.PYPI_API_TOKEN }}
results/petastorm/.github/workflows/unittest.yml:175 - password: ${{ secrets.PYPI_API_TOKEN }}
